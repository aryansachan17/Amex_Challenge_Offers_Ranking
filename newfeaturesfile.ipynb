{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12390030,"sourceType":"datasetVersion","datasetId":7780527}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-06T15:58:48.602075Z","iopub.execute_input":"2025-07-06T15:58:48.602542Z","iopub.status.idle":"2025-07-06T15:58:48.632456Z","shell.execute_reply.started":"2025-07-06T15:58:48.602515Z","shell.execute_reply":"2025-07-06T15:58:48.631282Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/amex-cleaned-datasets/test_data.parquet\n/kaggle/input/amex-cleaned-datasets/train_data_cleaned.parquet\n/kaggle/input/amex-cleaned-datasets/add_events_features2.parquet\n/kaggle/input/amex-cleaned-datasets/test_data_enhanced.parquet\n/kaggle/input/amex-cleaned-datasets/add_events_features.parquet\n/kaggle/input/amex-cleaned-datasets/train_data_capped_cleaned.parquet\n/kaggle/input/amex-cleaned-datasets/trainmodeltraining.ipynb\n/kaggle/input/amex-cleaned-datasets/add_trans.parquet\n/kaggle/input/amex-cleaned-datasets/agg_trans_features.parquet\n/kaggle/input/amex-cleaned-datasets/offer_clean.parquet\n/kaggle/input/amex-cleaned-datasets/train_data_enhanced.parquet\n/kaggle/input/amex-cleaned-datasets/train_data_merged.parquet\n/kaggle/input/amex-cleaned-datasets/testdownloadkarnekeliye.ipynb\n/kaggle/input/amex-cleaned-datasets/add_event_cleaned.parquet\n/kaggle/input/amex-cleaned-datasets/offermetadata_features.parquet\n/kaggle/input/amex-cleaned-datasets/cleaned_test_dataa.parquet\n/kaggle/input/amex-cleaned-datasets/test_data_cleaned.parquet\n/kaggle/input/amex-cleaned-datasets/train_data.parquet\n/kaggle/input/amex-cleaned-datasets/test_data_merged.parquet\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import kagglehub\n\n# Correct handle\npath = kagglehub.dataset_download(\"aryansachannn/amex-cleaned-datasets\")\n\nprint(\"✅ Dataset mounted at:\", path)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T15:58:48.634145Z","iopub.execute_input":"2025-07-06T15:58:48.634510Z","iopub.status.idle":"2025-07-06T15:58:48.820167Z","shell.execute_reply.started":"2025-07-06T15:58:48.634474Z","shell.execute_reply":"2025-07-06T15:58:48.819053Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset mounted at: /kaggle/input/amex-cleaned-datasets\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\n\ndef load_data():\n    \"\"\"Load all datasets with proper data type handling\"\"\"\n    print(\"Loading datasets...\")\n    \n    \n    # ✅ Corrected paths\n    train_data = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/train_data_cleaned.parquet')\n    test_data = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/test_data_cleaned.parquet')  # ✅ fixed typo\n    \n    # Load additional datasets\n    add_events = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/add_event_cleaned.parquet')\n    add_trans = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/add_trans.parquet')\n    offer_metadata = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/offer_clean.parquet')\n\n    \n    # Convert id3 to consistent data type (int64) - Handle NaN values first\n    train_data['id3'] = pd.to_numeric(train_data['id3'], errors='coerce').fillna(0).astype('int64')\n    test_data['id3'] = pd.to_numeric(test_data['id3'], errors='coerce').fillna(0).astype('int64')\n    offer_metadata['id3'] = pd.to_numeric(offer_metadata['id3'], errors='coerce').fillna(0).astype('int64')\n    add_events['id3'] = pd.to_numeric(add_events['id3'], errors='coerce').fillna(0).astype('int64')\n    \n    # Convert id2 to consistent data type (int64) - Handle NaN values first\n    train_data['id2'] = pd.to_numeric(train_data['id2'], errors='coerce').fillna(0).astype('int64')\n    test_data['id2'] = pd.to_numeric(test_data['id2'], errors='coerce').fillna(0).astype('int64')\n    add_events['id2'] = pd.to_numeric(add_events['id2'], errors='coerce').fillna(0).astype('int64')\n    add_trans['id2'] = pd.to_numeric(add_trans['id2'], errors='coerce').fillna(0).astype('int64')\n    \n    print(f\"Train data shape: {train_data.shape}\")\n    print(f\"Test data shape: {test_data.shape}\")\n    print(f\"Events data shape: {add_events.shape}\")\n    print(f\"Transaction data shape: {add_trans.shape}\")\n    print(f\"Offer metadata shape: {offer_metadata.shape}\")\n    \n    # Force garbage collection after loading\n    gc.collect()\n    \n    return train_data, test_data, add_events, add_trans, offer_metadata\n\ndef check_id_overlap(train_data, test_data, add_events, add_trans, offer_metadata):\n    \"\"\"Check for ID overlaps between datasets\"\"\"\n    print(\"\\nChecking ID overlaps...\")\n    \n    # Use sample if datasets are too large\n    train_sample = train_data.sample(n=min(100000, len(train_data)), random_state=42)\n    \n    train_id2 = set(train_sample['id2'].unique())\n    train_id3 = set(train_sample['id3'].unique())\n    \n    events_id2 = set(add_events['id2'].dropna().unique())\n    events_id3 = set(add_events['id3'].dropna().unique())\n    \n    trans_id2 = set(add_trans['id2'].dropna().unique())\n    offers_id3 = set(offer_metadata['id3'].dropna().unique())\n    \n    overlaps = {\n        'train_events_id2': len(train_id2.intersection(events_id2)),\n        'train_events_id3': len(train_id3.intersection(events_id3)),\n        'train_trans_id2': len(train_id2.intersection(trans_id2)),\n        'train_offers_id3': len(train_id3.intersection(offers_id3))\n    }\n    \n    for key, value in overlaps.items():\n        print(f\"{key}: {value}\")\n    \n    return overlaps\n\ndef clean_numeric_column(df, column):\n    \"\"\"Clean numeric column by converting non-numeric values to NaN\"\"\"\n    if column in df.columns:\n        # Convert to numeric, coercing errors to NaN\n        df[column] = pd.to_numeric(df[column], errors='coerce')\n    return df\n\ndef create_temporal_features(df, timestamp_col='id4'):\n    \"\"\"Create temporal features from timestamp column with memory optimization\"\"\"\n    print(f\"Creating temporal features for {len(df)} rows...\")\n    \n    # Process in chunks to avoid memory issues\n    chunk_size = 100000\n    processed_chunks = []\n    \n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i+chunk_size].copy()\n        \n        # Convert timestamp to datetime if it's not already\n        if not pd.api.types.is_datetime64_any_dtype(chunk[timestamp_col]):\n            chunk['datetime'] = pd.to_datetime(chunk[timestamp_col], errors='coerce')\n        else:\n            chunk['datetime'] = chunk[timestamp_col]\n        \n        # Extract temporal features\n        chunk['hour'] = chunk['datetime'].dt.hour\n        chunk['day_of_week'] = chunk['datetime'].dt.dayofweek\n        chunk['day_of_month'] = chunk['datetime'].dt.day\n        chunk['month'] = chunk['datetime'].dt.month\n        chunk['is_weekend'] = (chunk['day_of_week'] >= 5).astype('int8')\n        chunk['is_business_hours'] = ((chunk['hour'] >= 9) & (chunk['hour'] <= 17)).astype('int8')\n        chunk['is_evening'] = ((chunk['hour'] >= 18) & (chunk['hour'] <= 23)).astype('int8')\n        chunk['is_night'] = ((chunk['hour'] >= 0) & (chunk['hour'] <= 6)).astype('int8')\n        \n        processed_chunks.append(chunk)\n        \n        # Force garbage collection after each chunk\n        gc.collect()\n    \n    result = pd.concat(processed_chunks, ignore_index=True)\n    del processed_chunks\n    gc.collect()\n    \n    return result\n\ndef create_offer_features(offer_metadata, train_data, test_data):\n    \"\"\"Create offer-based features with memory optimization\"\"\"\n    print(\"\\nCreating offer features...\")\n    \n    # Create copies\n    train_enhanced = train_data.copy()\n    test_enhanced = test_data.copy()\n    \n    # Clean offer metadata\n    offer_clean = offer_metadata.copy()\n    \n    # Clean numeric columns in offer metadata\n    numeric_cols = ['f375', 'f376']\n    for col in numeric_cols:\n        offer_clean = clean_numeric_column(offer_clean, col)\n    \n    # Handle datetime columns with error handling\n    if 'id12' in offer_clean.columns:\n        offer_clean['id12'] = pd.to_datetime(offer_clean['id12'], errors='coerce')\n    if 'id13' in offer_clean.columns:\n        offer_clean['id13'] = pd.to_datetime(offer_clean['id13'], errors='coerce')\n    \n    # Create offer aggregations with error handling\n    try:\n        offer_agg = offer_clean.groupby('id3').agg({\n            'f375': ['mean', 'max', 'min', 'count'],\n            'f376': ['mean', 'max', 'min', 'std'],\n            'id8': ['count', 'nunique'] if 'id8' in offer_clean.columns else ['count'],\n            'f374': ['count', 'nunique'] if 'f374' in offer_clean.columns else ['count']\n        }).reset_index()\n        \n        # Flatten column names\n        offer_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in offer_agg.columns[1:]]\n        \n        # Convert id3 to int64 for merging\n        offer_agg['id3'] = offer_agg['id3'].astype('int64')\n        \n        # Fill NaN values in offer_agg\n        offer_agg = offer_agg.fillna(0)\n        \n        # Convert to appropriate data types to save memory\n        float_cols = offer_agg.select_dtypes(include=['float64']).columns\n        offer_agg[float_cols] = offer_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating offer aggregations: {e}\")\n        # Create empty aggregation with minimal columns\n        offer_agg = pd.DataFrame({'id3': offer_clean['id3'].unique()})\n        offer_agg['offer_count'] = 1\n        offer_agg['id3'] = offer_agg['id3'].astype('int64')\n    \n    # Global offer statistics\n    global_offer_stats = {\n        'global_avg_f375': offer_clean['f375'].mean() if 'f375' in offer_clean.columns else 0,\n        'global_avg_f376': offer_clean['f376'].mean() if 'f376' in offer_clean.columns else 0,\n        'global_unique_offers': offer_clean['id3'].nunique(),\n        'global_total_offers': len(offer_clean)\n    }\n    \n    # Merge with train and test data\n    print(\"Merging offer features...\")\n    train_enhanced = train_enhanced.merge(offer_agg, on='id3', how='left')\n    test_enhanced = test_enhanced.merge(offer_agg, on='id3', how='left')\n    \n    # Check merge success\n    merged_count = len(train_enhanced) - train_enhanced[offer_agg.columns[1]].isna().sum()\n    print(f\"Successfully merged offer features for {merged_count} out of {len(train_enhanced)} train records\")\n    \n    # Force garbage collection\n    del offer_clean, offer_agg\n    gc.collect()\n    \n    return train_enhanced, test_enhanced, global_offer_stats\n\ndef create_event_features(add_events, train_data, test_data):\n    \"\"\"Create event-based features with memory optimization\"\"\"\n    print(\"\\nCreating event features...\")\n    \n    # Sample events if too large\n    if len(add_events) > 1000000:\n        print(f\"Sampling events data from {len(add_events)} to 1M rows\")\n        add_events = add_events.sample(n=1000000, random_state=42)\n    \n    # Add temporal features to events\n    events_temporal = create_temporal_features(add_events, 'id4')\n    \n    # Create click indicator\n    events_temporal['clicked'] = (events_temporal['id7'].notna()).astype('int8')\n    \n    # Customer-level aggregations with error handling\n    try:\n        customer_event_agg = events_temporal.groupby('id2').agg({\n            'clicked': ['sum', 'mean', 'count'],\n            'hour': ['mean', 'std'],\n            'is_weekend': 'mean',\n            'is_business_hours': 'mean',\n            'is_evening': 'mean',\n            'id6': ['count', 'nunique'] if 'id6' in events_temporal.columns else ['count']\n        }).reset_index()\n        \n        # Flatten column names\n        customer_event_agg.columns = ['id2'] + [f'customer_{col[0]}_{col[1]}' for col in customer_event_agg.columns[1:]]\n        \n        # Convert to appropriate data types\n        float_cols = customer_event_agg.select_dtypes(include=['float64']).columns\n        customer_event_agg[float_cols] = customer_event_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating customer event aggregations: {e}\")\n        # Create minimal aggregation\n        customer_event_agg = events_temporal.groupby('id2').agg({\n            'clicked': ['sum', 'mean', 'count']\n        }).reset_index()\n        customer_event_agg.columns = ['id2', 'customer_total_clicks', 'customer_click_rate', 'customer_total_events']\n    \n    # Offer-level aggregations with error handling\n    try:\n        offer_event_agg = events_temporal.groupby('id3').agg({\n            'clicked': ['sum', 'mean', 'count'],\n            'hour': ['mean', 'std'],\n            'is_weekend': 'mean',\n            'is_business_hours': 'mean',\n            'id2': 'nunique'\n        }).reset_index()\n        \n        # Flatten column names\n        offer_event_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in offer_event_agg.columns[1:]]\n        \n        # Convert to appropriate data types\n        float_cols = offer_event_agg.select_dtypes(include=['float64']).columns\n        offer_event_agg[float_cols] = offer_event_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating offer event aggregations: {e}\")\n        # Create minimal aggregation\n        offer_event_agg = events_temporal.groupby('id3').agg({\n            'clicked': ['sum', 'mean', 'count']\n        }).reset_index()\n        offer_event_agg.columns = ['id3', 'offer_total_clicks', 'offer_click_rate', 'offer_total_events']\n    \n    # Global event statistics\n    global_event_stats = {\n        'global_click_rate': events_temporal['clicked'].mean(),\n        'global_avg_hour': events_temporal['hour'].mean(),\n        'global_weekend_rate': events_temporal['is_weekend'].mean(),\n        'global_business_hours_rate': events_temporal['is_business_hours'].mean(),\n        'global_total_events': len(events_temporal),\n        'global_total_clicks': int(events_temporal['clicked'].sum())\n    }\n    \n    # Simplified placement stats\n    placement_dict = {'global_placement_count': events_temporal['id6'].nunique() if 'id6' in events_temporal.columns else 0}\n    \n    # Fill NaN values and convert IDs\n    customer_event_agg = customer_event_agg.fillna(0)\n    offer_event_agg = offer_event_agg.fillna(0)\n    customer_event_agg['id2'] = customer_event_agg['id2'].astype('int64')\n    offer_event_agg['id3'] = offer_event_agg['id3'].astype('int64')\n    \n    # Force garbage collection\n    del events_temporal\n    gc.collect()\n    \n    return customer_event_agg, offer_event_agg, global_event_stats, placement_dict\n\ndef create_transaction_features(add_trans, train_data, test_data):\n    \"\"\"Create transaction-based features with memory optimization\"\"\"\n    print(\"\\nCreating transaction features...\")\n    \n    # Sample transactions if too large\n    if len(add_trans) > 2000000:\n        print(f\"Sampling transaction data from {len(add_trans)} to 2M rows\")\n        add_trans = add_trans.sample(n=2000000, random_state=42)\n    \n    # Clean transaction data first\n    trans_clean = add_trans.copy()\n    \n    # Clean numeric columns\n    numeric_cols = ['f367', 'f369']\n    for col in numeric_cols:\n        if col in trans_clean.columns:\n            trans_clean = clean_numeric_column(trans_clean, col)\n    \n    # Remove rows where critical numeric columns are NaN\n    print(f\"Original transaction data shape: {trans_clean.shape}\")\n    if 'f367' in trans_clean.columns:\n        trans_clean = trans_clean.dropna(subset=['f367'])\n        print(f\"After removing NaN amounts: {trans_clean.shape}\")\n    \n    # Add temporal features with error handling\n    try:\n        trans_temporal = create_temporal_features(trans_clean, 'f371')\n    except Exception as e:\n        print(f\"Error creating temporal features for transactions: {e}\")\n        # Use original data without temporal features\n        trans_temporal = trans_clean.copy()\n        trans_temporal['hour'] = 12  # Default hour\n        trans_temporal['is_weekend'] = 0\n        trans_temporal['is_business_hours'] = 1\n    \n    # Customer-level transaction aggregations with error handling\n    try:\n        agg_dict = {\n            'f367': ['sum', 'mean', 'std', 'count', 'max', 'min'],\n            'hour': ['mean', 'std'],\n            'is_weekend': 'mean',\n            'is_business_hours': 'mean'\n        }\n        \n        # Add f369 if it exists\n        if 'f369' in trans_temporal.columns:\n            agg_dict['f369'] = ['sum', 'mean', 'count']\n        \n        # Add f374 if it exists\n        if 'f374' in trans_temporal.columns:\n            agg_dict['f374'] = ['count', 'nunique']\n        \n        customer_trans_agg = trans_temporal.groupby('id2').agg(agg_dict).reset_index()\n        \n        # Flatten column names\n        customer_trans_agg.columns = ['id2'] + [f'customer_{col[0]}_{col[1]}' for col in customer_trans_agg.columns[1:]]\n        \n        # Convert to appropriate data types\n        float_cols = customer_trans_agg.select_dtypes(include=['float64']).columns\n        customer_trans_agg[float_cols] = customer_trans_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating customer transaction aggregations: {e}\")\n        # Create minimal aggregation\n        customer_trans_agg = trans_temporal.groupby('id2').agg({\n            'f367': ['sum', 'mean', 'count']\n        }).reset_index()\n        customer_trans_agg.columns = ['id2', 'customer_total_amount', 'customer_avg_amount', 'customer_total_transactions']\n    \n    # Global transaction statistics\n    global_trans_stats = {\n        'global_avg_transaction_amount': trans_temporal['f367'].mean() if 'f367' in trans_temporal.columns else 0,\n        'global_total_transactions': len(trans_temporal),\n        'global_avg_trans_hour': trans_temporal['hour'].mean() if 'hour' in trans_temporal.columns else 12,\n        'global_trans_weekend_rate': trans_temporal['is_weekend'].mean() if 'is_weekend' in trans_temporal.columns else 0.28,\n        'global_trans_business_hours_rate': trans_temporal['is_business_hours'].mean() if 'is_business_hours' in trans_temporal.columns else 0.33,\n        'global_unique_industries': trans_temporal['f374'].nunique() if 'f374' in trans_temporal.columns else 0\n    }\n    \n    # Simplified industry stats\n    industry_dict = {'global_industry_count': trans_temporal['f374'].nunique() if 'f374' in trans_temporal.columns else 0}\n    \n    # Fill NaN values and convert IDs\n    customer_trans_agg = customer_trans_agg.fillna(0)\n    customer_trans_agg['id2'] = customer_trans_agg['id2'].astype('int64')\n    \n    # Force garbage collection\n    del trans_clean, trans_temporal\n    gc.collect()\n    \n    return customer_trans_agg, global_trans_stats, industry_dict\n\ndef merge_all_features(train_enhanced, test_enhanced, customer_event_agg, offer_event_agg,\n                      customer_trans_agg, global_event_stats, global_trans_stats, \n                      global_offer_stats, placement_stats, industry_stats):\n    \"\"\"Merge all features into final datasets with memory optimization\"\"\"\n    print(\"\\nMerging all features...\")\n    \n    # Merge customer event features\n    print(\"Merging customer event features...\")\n    train_enhanced = train_enhanced.merge(customer_event_agg, on='id2', how='left')\n    test_enhanced = test_enhanced.merge(customer_event_agg, on='id2', how='left')\n    gc.collect()\n    \n    # Merge offer event features\n    print(\"Merging offer event features...\")\n    train_enhanced = train_enhanced.merge(offer_event_agg, on='id3', how='left')\n    test_enhanced = test_enhanced.merge(offer_event_agg, on='id3', how='left')\n    gc.collect()\n    \n    # Merge customer transaction features\n    print(\"Merging customer transaction features...\")\n    train_enhanced = train_enhanced.merge(customer_trans_agg, on='id2', how='left')\n    test_enhanced = test_enhanced.merge(customer_trans_agg, on='id2', how='left')\n    gc.collect()\n    \n    # Add temporal features to main datasets\n    print(\"Adding temporal features...\")\n    train_enhanced = create_temporal_features(train_enhanced, 'id4')\n    test_enhanced = create_temporal_features(test_enhanced, 'id4')\n    gc.collect()\n    \n    # Add global statistics as features\n    print(\"Adding global statistics...\")\n    all_global_stats = {**global_event_stats, **global_trans_stats, **global_offer_stats, \n                       **placement_stats, **industry_stats}\n    \n    for key, value in all_global_stats.items():\n        train_enhanced[key] = value\n        test_enhanced[key] = value\n    \n    # Fill NaN values with 0 for numeric columns\n    print(\"Filling NaN values...\")\n    numeric_cols = train_enhanced.select_dtypes(include=[np.number]).columns\n    train_enhanced[numeric_cols] = train_enhanced[numeric_cols].fillna(0)\n    test_enhanced[numeric_cols] = test_enhanced[numeric_cols].fillna(0)\n    \n    # Drop intermediate datetime column\n    train_enhanced = train_enhanced.drop('datetime', axis=1, errors='ignore')\n    test_enhanced = test_enhanced.drop('datetime', axis=1, errors='ignore')\n    \n    # Convert float64 to float32 to save memory\n    float64_cols = train_enhanced.select_dtypes(include=['float64']).columns\n    train_enhanced[float64_cols] = train_enhanced[float64_cols].astype('float32')\n    test_enhanced[float64_cols] = test_enhanced[float64_cols].astype('float32')\n    \n    gc.collect()\n    \n    return train_enhanced, test_enhanced\n\ndef main():\n    \"\"\"Main function to execute the entire pipeline with memory optimization\"\"\"\n    print(\"Starting Amex Offerings Feature Engineering Pipeline...\")\n    \n    try:\n        # Load data\n        train_data, test_data, add_events, add_trans, offer_metadata = load_data()\n        \n        # Check ID overlaps\n        overlaps = check_id_overlap(train_data, test_data, add_events, add_trans, offer_metadata)\n        \n        # Create offer features\n        train_enhanced, test_enhanced, global_offer_stats = create_offer_features(\n            offer_metadata, train_data, test_data)\n        \n        # Force garbage collection\n        del offer_metadata\n        gc.collect()\n        \n        # Create event features\n        customer_event_agg, offer_event_agg, global_event_stats, placement_stats = create_event_features(\n            add_events, train_data, test_data)\n        \n        # Force garbage collection\n        del add_events\n        gc.collect()\n        \n        # Create transaction features\n        customer_trans_agg, global_trans_stats, industry_stats = create_transaction_features(\n            add_trans, train_data, test_data)\n        \n        # Force garbage collection\n        del add_trans, train_data, test_data\n        gc.collect()\n        \n        # Merge all features\n        train_final, test_final = merge_all_features(\n            train_enhanced, test_enhanced, customer_event_agg, offer_event_agg,\n            customer_trans_agg, global_event_stats, global_trans_stats, \n            global_offer_stats, placement_stats, industry_stats)\n        \n        # Force garbage collection\n        del train_enhanced, test_enhanced, customer_event_agg, offer_event_agg, customer_trans_agg\n        gc.collect()\n        \n        # Print summary\n        print(f\"\\nFinal train data shape: {train_final.shape}\")\n        print(f\"Final test data shape: {test_final.shape}\")\n        \n        # Save enhanced datasets\n        print(\"\\nSaving enhanced datasets...\")\n        train_final.to_parquet('train_data_enhanced_final.parquet', index=False)\n        test_final.to_parquet('test_data_enhanced_final.parquet', index=False)\n        \n        print(\"\\nEnhanced datasets saved successfully!\")\n        print(\"- train_data_enhanced_final.parquet\")\n        print(\"- test_data_enhanced_final.parquet\")\n        \n        # Check for any remaining NaN values\n        print(f\"\\nRemaining NaN values in train: {train_final.isna().sum().sum()}\")\n        print(f\"Remaining NaN values in test: {test_final.isna().sum().sum()}\")\n        \n        print(\"\\nPipeline completed successfully!\")\n        \n        return train_final, test_final\n        \n    except Exception as e:\n        print(f\"Error in main pipeline: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\nif __name__ == \"__main__\":\n    train_enhanced, test_enhanced = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T15:58:48.822072Z","iopub.execute_input":"2025-07-06T15:58:48.822399Z","iopub.status.idle":"2025-07-06T16:00:30.802585Z","shell.execute_reply.started":"2025-07-06T15:58:48.822374Z","shell.execute_reply":"2025-07-06T16:00:30.801422Z"}},"outputs":[{"name":"stdout","text":"Starting Amex Offerings Feature Engineering Pipeline...\nLoading datasets...\nTrain data shape: (770164, 372)\nTest data shape: (369301, 371)\nEvents data shape: (21457473, 5)\nTransaction data shape: (6339465, 9)\nOffer metadata shape: (4164, 9)\n\nChecking ID overlaps...\ntrain_events_id2: 0\ntrain_events_id3: 713\ntrain_trans_id2: 0\ntrain_offers_id3: 709\n\nCreating offer features...\nMerging offer features...\nSuccessfully merged offer features for 770064 out of 770164 train records\n\nCreating event features...\nSampling events data from 21457473 to 1M rows\nCreating temporal features for 1000000 rows...\n\nCreating transaction features...\nSampling transaction data from 6339465 to 2M rows\nOriginal transaction data shape: (2000000, 9)\nAfter removing NaN amounts: (2000000, 9)\nCreating temporal features for 2000000 rows...\n\nMerging all features...\nMerging customer event features...\nMerging offer event features...\nMerging customer transaction features...\nAdding temporal features...\nCreating temporal features for 770164 rows...\nCreating temporal features for 369301 rows...\nAdding global statistics...\nFilling NaN values...\n\nFinal train data shape: (770164, 443)\nFinal test data shape: (369301, 442)\n\nSaving enhanced datasets...\n\nEnhanced datasets saved successfully!\n- train_data_enhanced_final.parquet\n- test_data_enhanced_final.parquet\n\nRemaining NaN values in train: 0\nRemaining NaN values in test: 0\n\nPipeline completed successfully!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\n\ndef load_data():\n    \"\"\"Load all datasets with proper data type handling\"\"\"\n    print(\"Loading datasets...\")\n    \n    # Load main datasets\n    train_data = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/train_data_capped_cleaned.parquet')\n    test_data = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/cleaned_test_dataa.parquet')\n    \n    # Load additional datasets\n    add_events = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/add_event_cleaned.parquet')\n    add_trans = pd.read_parquet('/kaggle/input/amex-cleaned-datasets/add_trans.parquet')\n    offer_metadata = pd.read_parquet('offer_clean.parquet')\n    \n    \n    # Convert id3 to consistent data type (int64) - Handle NaN values first\n    train_data['id3'] = pd.to_numeric(train_data['id3'], errors='coerce').fillna(0).astype('int64')\n    test_data['id3'] = pd.to_numeric(test_data['id3'], errors='coerce').fillna(0).astype('int64')\n    offer_metadata['id3'] = pd.to_numeric(offer_metadata['id3'], errors='coerce').fillna(0).astype('int64')\n    add_events['id3'] = pd.to_numeric(add_events['id3'], errors='coerce').fillna(0).astype('int64')\n    \n    # Convert id2 to consistent data type (int64) - Handle NaN values first\n    train_data['id2'] = pd.to_numeric(train_data['id2'], errors='coerce').fillna(0).astype('int64')\n    test_data['id2'] = pd.to_numeric(test_data['id2'], errors='coerce').fillna(0).astype('int64')\n    add_events['id2'] = pd.to_numeric(add_events['id2'], errors='coerce').fillna(0).astype('int64')\n    add_trans['id2'] = pd.to_numeric(add_trans['id2'], errors='coerce').fillna(0).astype('int64')\n    \n    print(f\"Train data shape: {train_data.shape}\")\n    print(f\"Test data shape: {test_data.shape}\")\n    print(f\"Events data shape: {add_events.shape}\")\n    print(f\"Transaction data shape: {add_trans.shape}\")\n    print(f\"Offer metadata shape: {offer_metadata.shape}\")\n    \n    # Force garbage collection after loading\n    gc.collect()\n    \n    return train_data, test_data, add_events, add_trans, offer_metadata\n\ndef check_id_overlap(train_data, test_data, add_events, add_trans, offer_metadata):\n    \"\"\"Check for ID overlaps between datasets\"\"\"\n    print(\"\\nChecking ID overlaps...\")\n    \n    # Use sample if datasets are too large\n    train_sample = train_data.sample(n=min(100000, len(train_data)), random_state=42)\n    \n    train_id2 = set(train_sample['id2'].unique())\n    train_id3 = set(train_sample['id3'].unique())\n    \n    events_id2 = set(add_events['id2'].dropna().unique())\n    events_id3 = set(add_events['id3'].dropna().unique())\n    \n    trans_id2 = set(add_trans['id2'].dropna().unique())\n    offers_id3 = set(offer_metadata['id3'].dropna().unique())\n    \n    overlaps = {\n        'train_events_id2': len(train_id2.intersection(events_id2)),\n        'train_events_id3': len(train_id3.intersection(events_id3)),\n        'train_trans_id2': len(train_id2.intersection(trans_id2)),\n        'train_offers_id3': len(train_id3.intersection(offers_id3))\n    }\n    \n    for key, value in overlaps.items():\n        print(f\"{key}: {value}\")\n    \n    return overlaps\n\ndef clean_numeric_column(df, column):\n    \"\"\"Clean numeric column by converting non-numeric values to NaN\"\"\"\n    if column in df.columns:\n        # Convert to numeric, coercing errors to NaN\n        df[column] = pd.to_numeric(df[column], errors='coerce')\n    return df\n\ndef create_temporal_features(df, timestamp_col='id4'):\n    \"\"\"Create temporal features from timestamp column with memory optimization\"\"\"\n    print(f\"Creating temporal features for {len(df)} rows...\")\n    \n    # Process in chunks to avoid memory issues\n    chunk_size = 100000\n    processed_chunks = []\n    \n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i+chunk_size].copy()\n        \n        # Convert timestamp to datetime if it's not already\n        if not pd.api.types.is_datetime64_any_dtype(chunk[timestamp_col]):\n            chunk['datetime'] = pd.to_datetime(chunk[timestamp_col], errors='coerce')\n        else:\n            chunk['datetime'] = chunk[timestamp_col]\n        \n        # Extract temporal features\n        chunk['hour'] = chunk['datetime'].dt.hour\n        chunk['day_of_week'] = chunk['datetime'].dt.dayofweek\n        chunk['day_of_month'] = chunk['datetime'].dt.day\n        chunk['month'] = chunk['datetime'].dt.month\n        chunk['is_weekend'] = (chunk['day_of_week'] >= 5).astype('int8')\n        chunk['is_business_hours'] = ((chunk['hour'] >= 9) & (chunk['hour'] <= 17)).astype('int8')\n        chunk['is_evening'] = ((chunk['hour'] >= 18) & (chunk['hour'] <= 23)).astype('int8')\n        chunk['is_night'] = ((chunk['hour'] >= 0) & (chunk['hour'] <= 6)).astype('int8')\n        \n        processed_chunks.append(chunk)\n        \n        # Force garbage collection after each chunk\n        gc.collect()\n    \n    result = pd.concat(processed_chunks, ignore_index=True)\n    del processed_chunks\n    gc.collect()\n    \n    return result\n\ndef create_offer_features(offer_metadata, train_data, test_data):\n    \"\"\"Create offer-based features with memory optimization\"\"\"\n    print(\"\\nCreating offer features...\")\n    \n    # Create copies\n    train_enhanced = train_data.copy()\n    test_enhanced = test_data.copy()\n    \n    # Clean offer metadata\n    offer_clean = offer_metadata.copy()\n    \n    # Clean numeric columns in offer metadata\n    numeric_cols = ['f375', 'f376']\n    for col in numeric_cols:\n        offer_clean = clean_numeric_column(offer_clean, col)\n    \n    # Handle datetime columns with error handling\n    if 'id12' in offer_clean.columns:\n        offer_clean['id12'] = pd.to_datetime(offer_clean['id12'], errors='coerce')\n    if 'id13' in offer_clean.columns:\n        offer_clean['id13'] = pd.to_datetime(offer_clean['id13'], errors='coerce')\n    \n    # Create offer aggregations with error handling\n    try:\n        offer_agg = offer_clean.groupby('id3').agg({\n            'f375': ['mean', 'max', 'min', 'count'],\n            'f376': ['mean', 'max', 'min', 'std'],\n            'id8': ['count', 'nunique'] if 'id8' in offer_clean.columns else ['count'],\n            'f374': ['count', 'nunique'] if 'f374' in offer_clean.columns else ['count']\n        }).reset_index()\n        \n        # Flatten column names\n        offer_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in offer_agg.columns[1:]]\n        \n        # Convert id3 to int64 for merging\n        offer_agg['id3'] = offer_agg['id3'].astype('int64')\n        \n        # Fill NaN values in offer_agg\n        offer_agg = offer_agg.fillna(0)\n        \n        # Convert to appropriate data types to save memory\n        float_cols = offer_agg.select_dtypes(include=['float64']).columns\n        offer_agg[float_cols] = offer_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating offer aggregations: {e}\")\n        # Create empty aggregation with minimal columns\n        offer_agg = pd.DataFrame({'id3': offer_clean['id3'].unique()})\n        offer_agg['offer_count'] = 1\n        offer_agg['id3'] = offer_agg['id3'].astype('int64')\n    \n    # Merge with train and test data\n    print(\"Merging offer features...\")\n    train_enhanced = train_enhanced.merge(offer_agg, on='id3', how='left')\n    test_enhanced = test_enhanced.merge(offer_agg, on='id3', how='left')\n    \n    # Check merge success\n    merged_count = len(train_enhanced) - train_enhanced[offer_agg.columns[1]].isna().sum()\n    print(f\"Successfully merged offer features for {merged_count} out of {len(train_enhanced)} train records\")\n    \n    # Force garbage collection\n    del offer_clean, offer_agg\n    gc.collect()\n    \n    return train_enhanced, test_enhanced\n\ndef create_event_features(add_events, train_data, test_data):\n    \"\"\"Create event-based features with memory optimization\"\"\"\n    print(\"\\nCreating event features...\")\n    \n    # Sample events if too large\n    if len(add_events) > 1000000:\n        print(f\"Sampling events data from {len(add_events)} to 1M rows\")\n        add_events = add_events.sample(n=1000000, random_state=42)\n    \n    # Add temporal features to events\n    events_temporal = create_temporal_features(add_events, 'id4')\n    \n    # Create click indicator\n    events_temporal['clicked'] = (events_temporal['id7'].notna()).astype('int8')\n    \n    # Customer-level aggregations with error handling\n    try:\n        customer_event_agg = events_temporal.groupby('id2').agg({\n            'clicked': ['sum', 'mean', 'count'],\n            'hour': ['mean', 'std'],\n            'is_weekend': 'mean',\n            'is_business_hours': 'mean',\n            'is_evening': 'mean',\n            'id6': ['count', 'nunique'] if 'id6' in events_temporal.columns else ['count']\n        }).reset_index()\n        \n        # Flatten column names\n        customer_event_agg.columns = ['id2'] + [f'customer_{col[0]}_{col[1]}' for col in customer_event_agg.columns[1:]]\n        \n        # Convert to appropriate data types\n        float_cols = customer_event_agg.select_dtypes(include=['float64']).columns\n        customer_event_agg[float_cols] = customer_event_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating customer event aggregations: {e}\")\n        # Create minimal aggregation\n        customer_event_agg = events_temporal.groupby('id2').agg({\n            'clicked': ['sum', 'mean', 'count']\n        }).reset_index()\n        customer_event_agg.columns = ['id2', 'customer_total_clicks', 'customer_click_rate', 'customer_total_events']\n    \n    # Offer-level aggregations with error handling\n    try:\n        offer_event_agg = events_temporal.groupby('id3').agg({\n            'clicked': ['sum', 'mean', 'count'],\n            'hour': ['mean', 'std'],\n            'is_weekend': 'mean',\n            'is_business_hours': 'mean',\n            'id2': 'nunique'\n        }).reset_index()\n        \n        # Flatten column names\n        offer_event_agg.columns = ['id3'] + [f'offer_{col[0]}_{col[1]}' for col in offer_event_agg.columns[1:]]\n        \n        # Convert to appropriate data types\n        float_cols = offer_event_agg.select_dtypes(include=['float64']).columns\n        offer_event_agg[float_cols] = offer_event_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating offer event aggregations: {e}\")\n        # Create minimal aggregation\n        offer_event_agg = events_temporal.groupby('id3').agg({\n            'clicked': ['sum', 'mean', 'count']\n        }).reset_index()\n        offer_event_agg.columns = ['id3', 'offer_total_clicks', 'offer_click_rate', 'offer_total_events']\n    \n    # Force garbage collection\n    del events_temporal\n    gc.collect()\n    \n    return customer_event_agg, offer_event_agg\n\ndef create_transaction_features(add_trans, train_data, test_data):\n    \"\"\"Create transaction-based features with memory optimization\"\"\"\n    print(\"\\nCreating transaction features...\")\n    \n    # Sample transactions if too large\n    if len(add_trans) > 2000000:\n        print(f\"Sampling transaction data from {len(add_trans)} to 2M rows\")\n        add_trans = add_trans.sample(n=2000000, random_state=42)\n    \n    # Clean transaction data first\n    trans_clean = add_trans.copy()\n    \n    # Clean numeric columns\n    numeric_cols = ['f367', 'f369']\n    for col in numeric_cols:\n        if col in trans_clean.columns:\n            trans_clean = clean_numeric_column(trans_clean, col)\n    \n    # Remove rows where critical numeric columns are NaN\n    print(f\"Original transaction data shape: {trans_clean.shape}\")\n    if 'f367' in trans_clean.columns:\n        trans_clean = trans_clean.dropna(subset=['f367'])\n        print(f\"After removing NaN amounts: {trans_clean.shape}\")\n    \n    # Add temporal features with error handling\n    try:\n        trans_temporal = create_temporal_features(trans_clean, 'f371')\n    except Exception as e:\n        print(f\"Error creating temporal features for transactions: {e}\")\n        # Use original data without temporal features\n        trans_temporal = trans_clean.copy()\n        trans_temporal['hour'] = 12  # Default hour\n        trans_temporal['is_weekend'] = 0\n        trans_temporal['is_business_hours'] = 1\n    \n    # Customer-level transaction aggregations with error handling\n    try:\n        agg_dict = {\n            'f367': ['sum', 'mean', 'std', 'count', 'max', 'min'],\n            'hour': ['mean', 'std'],\n            'is_weekend': 'mean',\n            'is_business_hours': 'mean'\n        }\n        \n        # Add f369 if it exists\n        if 'f369' in trans_temporal.columns:\n            agg_dict['f369'] = ['sum', 'mean', 'count']\n        \n        # Add f374 if it exists\n        if 'f374' in trans_temporal.columns:\n            agg_dict['f374'] = ['count', 'nunique']\n        \n        customer_trans_agg = trans_temporal.groupby('id2').agg(agg_dict).reset_index()\n        \n        # Flatten column names\n        customer_trans_agg.columns = ['id2'] + [f'customer_{col[0]}_{col[1]}' for col in customer_trans_agg.columns[1:]]\n        \n        # Convert to appropriate data types\n        float_cols = customer_trans_agg.select_dtypes(include=['float64']).columns\n        customer_trans_agg[float_cols] = customer_trans_agg[float_cols].astype('float32')\n        \n    except Exception as e:\n        print(f\"Error creating customer transaction aggregations: {e}\")\n        # Create minimal aggregation\n        customer_trans_agg = trans_temporal.groupby('id2').agg({\n            'f367': ['sum', 'mean', 'count']\n        }).reset_index()\n        customer_trans_agg.columns = ['id2', 'customer_total_amount', 'customer_avg_amount', 'customer_total_transactions']\n    \n    # Fill NaN values and convert IDs\n    customer_trans_agg = customer_trans_agg.fillna(0)\n    customer_trans_agg['id2'] = customer_trans_agg['id2'].astype('int64')\n    \n    # Force garbage collection\n    del trans_clean, trans_temporal\n    gc.collect()\n    \n    return customer_trans_agg\n\ndef create_relative_features(train_enhanced, test_enhanced):\n    \"\"\"Create relative features that compare individual values to group averages\"\"\"\n    print(\"\\nCreating relative features...\")\n    \n    # Combine train and test to get overall statistics for relative features\n    combined = pd.concat([train_enhanced, test_enhanced], ignore_index=True)\n    \n    # Customer-level relative features\n    if 'customer_clicked_mean' in combined.columns:\n        overall_click_rate = combined['customer_clicked_mean'].mean()\n        train_enhanced['customer_click_rate_vs_avg'] = train_enhanced['customer_clicked_mean'] / (overall_click_rate + 1e-8)\n        test_enhanced['customer_click_rate_vs_avg'] = test_enhanced['customer_clicked_mean'] / (overall_click_rate + 1e-8)\n    \n    # Transaction amount relative features\n    if 'customer_f367_mean' in combined.columns:\n        overall_trans_avg = combined['customer_f367_mean'].mean()\n        train_enhanced['customer_trans_amount_vs_avg'] = train_enhanced['customer_f367_mean'] / (overall_trans_avg + 1e-8)\n        test_enhanced['customer_trans_amount_vs_avg'] = test_enhanced['customer_f367_mean'] / (overall_trans_avg + 1e-8)\n    \n    # Offer popularity relative features\n    if 'offer_clicked_sum' in combined.columns:\n        overall_offer_popularity = combined['offer_clicked_sum'].mean()\n        train_enhanced['offer_popularity_vs_avg'] = train_enhanced['offer_clicked_sum'] / (overall_offer_popularity + 1e-8)\n        test_enhanced['offer_popularity_vs_avg'] = test_enhanced['offer_clicked_sum'] / (overall_offer_popularity + 1e-8)\n    \n    # Hour-based relative features\n    if 'hour' in combined.columns:\n        hour_click_rates = combined.groupby('hour')['customer_clicked_mean'].mean()\n        train_enhanced['hour_click_rate_vs_hour_avg'] = train_enhanced.apply(\n            lambda x: x['customer_clicked_mean'] / (hour_click_rates.get(x['hour'], 0.1) + 1e-8), axis=1\n        )\n        test_enhanced['hour_click_rate_vs_hour_avg'] = test_enhanced.apply(\n            lambda x: x['customer_clicked_mean'] / (hour_click_rates.get(x['hour'], 0.1) + 1e-8), axis=1\n        )\n    \n    del combined\n    gc.collect()\n    \n    return train_enhanced, test_enhanced\n\ndef merge_all_features(train_enhanced, test_enhanced, customer_event_agg, offer_event_agg, customer_trans_agg):\n    \"\"\"Merge all features into final datasets with memory optimization\"\"\"\n    print(\"\\nMerging all features...\")\n    \n    # Merge customer event features\n    print(\"Merging customer event features...\")\n    train_enhanced = train_enhanced.merge(customer_event_agg, on='id2', how='left')\n    test_enhanced = test_enhanced.merge(customer_event_agg, on='id2', how='left')\n    gc.collect()\n    \n    # Merge offer event features\n    print(\"Merging offer event features...\")\n    train_enhanced = train_enhanced.merge(offer_event_agg, on='id3', how='left')\n    test_enhanced = test_enhanced.merge(offer_event_agg, on='id3', how='left')\n    gc.collect()\n    \n    # Merge customer transaction features\n    print(\"Merging customer transaction features...\")\n    train_enhanced = train_enhanced.merge(customer_trans_agg, on='id2', how='left')\n    test_enhanced = test_enhanced.merge(customer_trans_agg, on='id2', how='left')\n    gc.collect()\n    \n    # Add temporal features to main datasets\n    print(\"Adding temporal features...\")\n    train_enhanced = create_temporal_features(train_enhanced, 'id4')\n    test_enhanced = create_temporal_features(test_enhanced, 'id4')\n    gc.collect()\n    \n    # Create relative features instead of global constants\n    train_enhanced, test_enhanced = create_relative_features(train_enhanced, test_enhanced)\n    gc.collect()\n    \n    # Fill NaN values with 0 for numeric columns\n    print(\"Filling NaN values...\")\n    numeric_cols = train_enhanced.select_dtypes(include=[np.number]).columns\n    train_enhanced[numeric_cols] = train_enhanced[numeric_cols].fillna(0)\n    test_enhanced[numeric_cols] = test_enhanced[numeric_cols].fillna(0)\n    \n    # Drop intermediate datetime column\n    train_enhanced = train_enhanced.drop('datetime', axis=1, errors='ignore')\n    test_enhanced = test_enhanced.drop('datetime', axis=1, errors='ignore')\n    \n    # Convert float64 to float32 to save memory\n    float64_cols = train_enhanced.select_dtypes(include=['float64']).columns\n    train_enhanced[float64_cols] = train_enhanced[float64_cols].astype('float32')\n    test_enhanced[float64_cols] = test_enhanced[float64_cols].astype('float32')\n    \n    gc.collect()\n    \n    return train_enhanced, test_enhanced\n\ndef remove_constant_features(train_final, test_final):\n    \"\"\"Remove features that have the same value across all samples\"\"\"\n    print(\"\\nRemoving constant features...\")\n    \n    # Check for constant features in train\n    constant_features = []\n    for col in train_final.columns:\n        if col not in ['id1', 'id2', 'id3', 'id4', 'target']:  # Skip ID columns and target\n            if train_final[col].nunique() <= 1:\n                constant_features.append(col)\n    \n    # Also check for features that have the same single value in both train and test\n    same_value_features = []\n    for col in train_final.columns:\n        if col not in ['id1', 'id2', 'id3', 'id4', 'target']:  # Skip ID columns and target\n            if (train_final[col].nunique() == 1 and \n                test_final[col].nunique() == 1 and \n                train_final[col].iloc[0] == test_final[col].iloc[0]):\n                same_value_features.append(col)\n    \n    all_constant_features = list(set(constant_features + same_value_features))\n    \n    if all_constant_features:\n        print(f\"Removing {len(all_constant_features)} constant features:\")\n        print(all_constant_features)\n        \n        train_final = train_final.drop(columns=all_constant_features)\n        test_final = test_final.drop(columns=all_constant_features)\n    else:\n        print(\"No constant features found.\")\n    \n    return train_final, test_final\n\ndef main():\n    \"\"\"Main function to execute the entire pipeline with memory optimization\"\"\"\n    print(\"Starting Amex Offerings Feature Engineering Pipeline...\")\n    \n    try:\n        # Load data\n        train_data, test_data, add_events, add_trans, offer_metadata = load_data()\n        \n        # Check ID overlaps\n        overlaps = check_id_overlap(train_data, test_data, add_events, add_trans, offer_metadata)\n        \n        # Create offer features\n        train_enhanced, test_enhanced = create_offer_features(offer_metadata, train_data, test_data)\n        \n        # Force garbage collection\n        del offer_metadata\n        gc.collect()\n        \n        # Create event features\n        customer_event_agg, offer_event_agg = create_event_features(add_events, train_data, test_data)\n        \n        # Force garbage collection\n        del add_events\n        gc.collect()\n        \n        # Create transaction features\n        customer_trans_agg = create_transaction_features(add_trans, train_data, test_data)\n        \n        # Force garbage collection\n        del add_trans, train_data, test_data\n        gc.collect()\n        \n        # Merge all features\n        train_final, test_final = merge_all_features(\n            train_enhanced, test_enhanced, customer_event_agg, offer_event_agg, customer_trans_agg)\n        \n        # Force garbage collection\n        del train_enhanced, test_enhanced, customer_event_agg, offer_event_agg, customer_trans_agg\n        gc.collect()\n        \n        # Remove constant features\n        train_final, test_final = remove_constant_features(train_final, test_final)\n        \n        # Print summary\n        print(f\"\\nFinal train data shape: {train_final.shape}\")\n        print(f\"Final test data shape: {test_final.shape}\")\n        \n        # Save enhanced datasets\n        print(\"\\nSaving enhanced datasets...\")\n        train_final.to_parquet('train_data_enhanced_final.parquet', index=False)\n        test_final.to_parquet('test_data_enhanced_final.parquet', index=False)\n        \n        print(\"\\nEnhanced datasets saved successfully!\")\n        print(\"- train_data_enhanced_final.parquet\")\n        print(\"- test_data_enhanced_final.parquet\")\n        \n        # Check for any remaining NaN values\n        print(f\"\\nRemaining NaN values in train: {train_final.isna().sum().sum()}\")\n        print(f\"Remaining NaN values in test: {test_final.isna().sum().sum()}\")\n        \n        # Check for remaining constant features\n        print(f\"\\nFinal feature count: {len(train_final.columns)}\")\n        \n        print(\"\\nPipeline completed successfully!\")\n        \n        return train_final, test_final\n        \n    except Exception as e:\n        print(f\"Error in main pipeline: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\nif _name_ == \"_main_\":\n    train_enhanced, test_enhanced = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:00:30.804628Z","iopub.execute_input":"2025-07-06T16:00:30.804929Z","iopub.status.idle":"2025-07-06T16:00:30.855115Z","shell.execute_reply.started":"2025-07-06T16:00:30.804905Z","shell.execute_reply":"2025-07-06T16:00:30.853510Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/3652256343.py\"\u001b[0;36m, line \u001b[0;32m533\u001b[0m\n\u001b[0;31m    train_enhanced, test_enhanced = main()\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"],"ename":"SyntaxError","evalue":"invalid non-printable character U+00A0 (3652256343.py, line 533)","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"train_enhanced","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:00:30.856240Z","iopub.status.idle":"2025-07-06T16:00:30.856518Z","shell.execute_reply.started":"2025-07-06T16:00:30.856392Z","shell.execute_reply":"2025-07-06T16:00:30.856405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_enhanced ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:00:30.857608Z","iopub.status.idle":"2025-07-06T16:00:30.858014Z","shell.execute_reply.started":"2025-07-06T16:00:30.857797Z","shell.execute_reply":"2025-07-06T16:00:30.857814Z"}},"outputs":[],"execution_count":null}]}